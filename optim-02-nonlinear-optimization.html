
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Nonlinear Regression &#8212; Lecture Notes for Cornell Chem 7870</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'optim-02-nonlinear-optimization';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Automatic Differentiation" href="optim-03a-autograd-in-pytorch.html" />
    <link rel="prev" title="Optimization" href="optim-01-intro-optimization.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Lecture Notes for Cornell Chem 7870 - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="Lecture Notes for Cornell Chem 7870 - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to 7870
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1. Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="linalg-01-intro_vectors.html">A Deeper look at Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg-02-coding_matrices_in_python.html">Linear Algebra in Python with Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg-03-matrix_decompositions.html">Eigenvectors, Unitary Matrices, and Matrix Decompositions</a></li>
<li class="toctree-l1"><a class="reference internal" href="linalg-04-linear_regression.html">Linear Regression and Least Squares</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2. Optimization and Numerical Differential Equations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="optim-01-intro-optimization.html">Optimization</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim-03a-autograd-in-pytorch.html">Automatic Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="optim-03b-autograd-in-jax.html">JaX</a></li>



<li class="toctree-l1"><a class="reference internal" href="diffeq-01-ode-intro.html">Intro to Numerical Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffeq-02-symplectic-integrators.html">More on Numerical ODEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="diffeq-03-pde.html">From ODE to PDE</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3. Probability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="prob-01-probability.html">Intro to Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-02-random-in-numpy.html">Using Random Numbers in Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-03-bayes-rule.html">Conditional Probability and Bayes Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-04-bayesian-inference.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-05-monte-carlo.html">Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-06-mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="prob-07-metropolis-hastings.html">Detailed Balance and the Metropolis-Hastings Algorithm</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Foptim-02-nonlinear-optimization.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/optim-02-nonlinear-optimization.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Nonlinear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Nonlinear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-logistic-regression">Example: Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-to-neural-networks">Extension to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-density-estimation">Example: Density Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-learning-force-fields">Example: Learning Force Fields</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="nonlinear-regression">
<h1>Nonlinear Regression<a class="headerlink" href="#nonlinear-regression" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Discuss the general framework of Nonlinear Regression.</p></li>
<li><p>Discuss Logistic regression and its loss functional.</p></li>
<li><p>Give a brief overview of neural networks and their loss functionals.</p></li>
<li><p>Discuss density estimation.</p></li>
</ul>
</section>
<section id="id1">
<h2>Nonlinear Regression<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>In the beginning of this class, we repeatedly discussed linear regression: the act of determining a linear relation between two variables.
Here, we extend this to nonlinear regression.</p>
<p>In general, nonlinear regression has the following form.
We have some parameterized function <span class="math notranslate nohighlight">\(f(x, \theta)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> is a vector of parameters.
Additionally, we have a functional <span class="math notranslate nohighlight">\(L\)</span> that measures the quality for <span class="math notranslate nohighlight">\(f\)</span>, which we call the <em>loss function</em>.
Our goal is to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that minimize the loss functional,</p>
<div class="math notranslate nohighlight">
\[
\theta^* = \arg \min_\theta L(f(x, \theta), y)
\]</div>
<p>where <span class="math notranslate nohighlight">\(y\)</span> is the data we are trying to fit.
If <span class="math notranslate nohighlight">\(f\)</span> is differentiable with respect to <span class="math notranslate nohighlight">\(\theta\)</span>, we can use gradient descent or Newton’s method to find the minimum.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In the case of linear regression, we had <span class="math notranslate nohighlight">\(f(x, \theta) = Ax + b\)</span>, where <span class="math notranslate nohighlight">\(A\)</span> is a matrix and <span class="math notranslate nohighlight">\(b\)</span> is a vector.
The loss functional was the mean square <span class="math notranslate nohighlight">\(L(f(x, \theta), y) = ||Ax + b - y||^2\)</span>.</p>
</div>
<p>While accurate, the above definition is a bit abstract.  In this lecture, we discuss some more concrete examples.</p>
</section>
<section id="example-logistic-regression">
<h2>Example: Logistic Regression<a class="headerlink" href="#example-logistic-regression" title="Link to this heading">#</a></h2>
<p>Logistic regression is a common method for binary classification.
Here, each <span class="math notranslate nohighlight">\(x\)</span> is associated with one of two outcomes which we denote <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.
Our goal is to fit a function that predicts the probability of <span class="math notranslate nohighlight">\(x\)</span> being associated with outcome <span class="math notranslate nohighlight">\(1\)</span>.
To be able to use gradient descent, rather than directly predicting 0’s or 1’s we us a function that goes smoothly between the two: a sigmoid function.
The sigmoid function is given by:</p>
<div class="math notranslate nohighlight">
\[
    f(x, \theta) = \frac{1}{1 + e^{-\theta^T x}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\theta\)</span> is a vector of parameters.
For our loss functional, one option would have been to use the mean square error.  However, this can lead to some practical issues.  The biggest is that gradients can be very small.
To demonstrate this, let’s consider an example in one dimension.
We have a point at <span class="math notranslate nohighlight">\(x = 1\)</span>, and the true label is <span class="math notranslate nohighlight">\(y = 0\)</span>.  However, our current parameter is set to <span class="math notranslate nohighlight">\(\theta=10\)</span>.
Consequently, our prediction is set to</p>
<div class="math notranslate nohighlight">
\[
 \frac{1}{1 + e^{-10}}  \approx 0.9999546
\]</div>
<p>For this point, our mean square error is then given by <span class="math notranslate nohighlight">\((0.9999546 - 0)^2 \approx 0.9999092\)</span>.
To fix this, let’s halve  our parameter, setting it to <span class="math notranslate nohighlight">\(\theta=5\)</span>.
Now, our prediction is given by</p>
<div class="math notranslate nohighlight">
\[
 \frac{1}{1 + e^{-5}}  \approx 0.9933071
\]</div>
<p>Our mean square error is now <span class="math notranslate nohighlight">\((0.9933071 - 0)^2 \approx 0.9867\)</span>.  Despite the fact the we have halved our parameter, our mean square error has barely decreased!</p>
<p>Consequently, it is common to use a different loss functional, called the <em>cross-entropy loss</em>.
The cross-entropy loss is given by:</p>
<div class="math notranslate nohighlight">
\[
L(f(x, \theta), y) = -y \log(f(x, \theta)) - (1-y) \log(1-f(x, \theta))
\]</div>
<p>Note that since <span class="math notranslate nohighlight">\(y\)</span> is either <span class="math notranslate nohighlight">\(0\)</span> or <span class="math notranslate nohighlight">\(1\)</span>, this is equivalent to:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
L(f(x, \theta), y) = \begin{cases}
    -\log(f(x, \theta)) &amp; \text{if } y = 1 \\
    -\log(1-f(x, \theta)) &amp; \text{if } y = 0
\end{cases}
\end{split}\]</div>
<p>This loss function has much better properties than the mean square error.  The gradients when we are far from the true solution are much larger.  Moreover, it is convex, which helps with higher-order optimization methods.</p>
</section>
<section id="extension-to-neural-networks">
<h2>Extension to Neural Networks<a class="headerlink" href="#extension-to-neural-networks" title="Link to this heading">#</a></h2>
<p>In general, linear regression is a good go-to method for predicting continuous values, logistic regression is a good go-to method for predicting binary values.
However, sometimes we might want a more complex function than a linear function.
In this case we can employ a <em>neural network</em>.
At their most basic, neural networks are a class of functions that are composed of many layers of linear functions, each followed by a nonlinear function.
Denoting the nonlinear function as <span class="math notranslate nohighlight">\(\sigma\)</span>, we can write the neural network as</p>
<div class="math notranslate nohighlight">
\[
NN(x, \theta) =  \theta_n \sigma (\theta_{n-1} \sigma( \cdots \sigma( \theta_2 \sigma( \theta_1 x))))
\]</div>
<p>where we have divided our parameters into <span class="math notranslate nohighlight">\(n\)</span> matrices, each denoted <span class="math notranslate nohighlight">\(\theta_i\)</span>.
Different neural networks differ on the number of layers, if and how we constrain the parameters, and how we choose the nonlinear function <span class="math notranslate nohighlight">\(\sigma\)</span>.
Common choices for <span class="math notranslate nohighlight">\(\sigma\)</span> include the sigmoid function discussed above,
the hyperbolic tangent function</p>
<div class="math notranslate nohighlight">
\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}},
\]</div>
<p>and (perhaps the most popular option) the rectified linear unit (ReLU) function</p>
<div class="math notranslate nohighlight">
\[
\text{ReLU}(x) = \max(0, x)
\]</div>
<p>Once we have chosen the form for our neural network, we tune it to predict values.
For predicting continuous values, we use a linear loss functional</p>
<div class="math notranslate nohighlight">
\[
L(f(x, \theta), y) = ||f(x, \theta) - y||^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(||\cdot||\)</span> is the Euclidean norm.  For predicting binary values, we use the cross-entropy loss functional</p>
<div class="math notranslate nohighlight">
\[
L(f(x, \theta), y) = -y \log(f(x, \theta)) - (1-y) \log(1-f(x, \theta))
\]</div>
<p>where we set</p>
<div class="math notranslate nohighlight">
\[
f(x, \theta) = \frac{1}{1 + e^{-NN(x, \theta)}}s
\]</div>
<p>with <span class="math notranslate nohighlight">\(NN(x, \theta)\)</span> being the neural network we defined above.</p>
</section>
<section id="example-density-estimation">
<h2>Example: Density Estimation<a class="headerlink" href="#example-density-estimation" title="Link to this heading">#</a></h2>
<p>Given a set of samples <span class="math notranslate nohighlight">\(x_i\)</span> from some probability distribution drawn from an unknown probability density <span class="math notranslate nohighlight">\(\rho\)</span>, we might wish to recover an approximation of the underlying probability density.
This is known as <em>density estimation</em>.
One approach to solving this problem is to parameterize a set of possible probability densities <span class="math notranslate nohighlight">\(f(x, \theta)\)</span>, and write down a loss functional that measures the difference between the true probability density and our approximation.
As it happens, we can generalize the cross-entropy loss functional to do this.</p>
<p>For general probability densities <span class="math notranslate nohighlight">\(p(x)\)</span> and <span class="math notranslate nohighlight">\(q(x)\)</span>, the cross entropy is given by:</p>
<div class="math notranslate nohighlight">
\[
H(p, q) = -\int p(x) \log(q(x)) dx
\]</div>
<p>This is a measure of how well <span class="math notranslate nohighlight">\(q\)</span> approximates <span class="math notranslate nohighlight">\(p\)</span>: as <span class="math notranslate nohighlight">\(q\)</span> gets closer to <span class="math notranslate nohighlight">\(p\)</span>, the cross-entropy goes down.
We can use this to define a loss functional for density estimation by replacing <span class="math notranslate nohighlight">\(p\)</span> with the true probability density <span class="math notranslate nohighlight">\(\rho\)</span> and <span class="math notranslate nohighlight">\(q\)</span> with our approximation <span class="math notranslate nohighlight">\(f(x, \theta)\)</span>.
Substituting this into the cross-entropy, we get:</p>
<div class="math notranslate nohighlight">
\[
 -\int \rho(x) \log(f(x, \theta)) dx
\]</div>
<p>We can approximate this using an average over our samples to get a loss funcitonal that we can minimize:</p>
<div class="math notranslate nohighlight">
\[
L(f(x, \theta), y) = -\frac{1}{N} \sum_{i=1}^N \log(f(x_i, \theta))
\]</div>
<p>where <span class="math notranslate nohighlight">\(N\)</span> is the number of samples.
Our remaining task is to parameterize <span class="math notranslate nohighlight">\(f\)</span>.  One common approach is to use a <em>Gaussian mixture model</em> (GMM).
A GMM is a weighted sum of Gaussians, given by:</p>
<div class="math notranslate nohighlight">
\[
f(x, \theta) = \sum_{i=1}^K w_i \mathcal{N}(x | \mu_i, \Sigma_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(w_i\)</span> is the weight of the <span class="math notranslate nohighlight">\(i\)</span>th Gaussian, <span class="math notranslate nohighlight">\(\mu_i\)</span> is the mean of the <span class="math notranslate nohighlight">\(i\)</span>th Gaussian, and <span class="math notranslate nohighlight">\(\Sigma_i\)</span> is the covariance of the <span class="math notranslate nohighlight">\(i\)</span>th Gaussian.
The weights <span class="math notranslate nohighlight">\(w_i\)</span> are constrained to be non-negative and sum to 1, while the means <span class="math notranslate nohighlight">\(\mu_i\)</span> and covariances <span class="math notranslate nohighlight">\(\Sigma_i\)</span> can be any real-valued matrix.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Depending on the dimension of the data, gradient-based optimizers may not be the best choice for this problem.
If you are interested in pursuing this direction for your research, it may be worth looking into the <em>Expectation-Maximization</em> (EM) algorithm.</p>
</div>
</section>
<section id="example-learning-force-fields">
<h2>Example: Learning Force Fields<a class="headerlink" href="#example-learning-force-fields" title="Link to this heading">#</a></h2>
<p>Force fields are physical models that predict the potential energy of a system based on its configuration.
This potential energy is, in principle, computable from first principles using quantum mechanics.
However, this is often far to computationally expensive to be practical.  Instead, we approximate this energy by a sum of particle-particle interactions.  For instance, electrostatic interactions are given by the Coulomb potential:</p>
<div class="math notranslate nohighlight">
\[
V(r) = \frac{q_1 q_2}{r}
\]</div>
<p>where <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span> are the (effective) charges of the two particles, and <span class="math notranslate nohighlight">\(r\)</span> is the distance between them.
Interatomic repulsion and van der Waals interactions are given by the Lennard-Jones potential:</p>
<div class="math notranslate nohighlight">
\[
V(r) = 4 \epsilon \left( \left( \frac{\sigma}{r} \right)^{12} - \left( \frac{\sigma}{r} \right)^{6} \right)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is the depth of the potential well, and <span class="math notranslate nohighlight">\(\sigma\)</span> is the distance at which the potential is zero.
We might also have additional terms that depend on the angle between three particles, or the dihedral angle between four particles.
However, in all of these cases we need to tune parameters of these functions to approximate the quantum mechanical energies as well as possible.</p>
<p>This is a linear regression problem: we have a set of quantum energies <span class="math notranslate nohighlight">\(y\)</span>, a complicated function <span class="math notranslate nohighlight">\(f\)</span> that approximates them given a structure <span class="math notranslate nohighlight">\(x\)</span>, and we want to find the parameters that minimize the difference between the two.
Again, we can calculate the mean-squared in the predicted and the true energies.  But this often isn’t enough to get good force fields.
The reason for this is that even if a function converges, that doesn’t mean its derivative converges.
As a classic example, consider the function <span class="math notranslate nohighlight">\(1 / k \sin(k x)\)</span>.  As <span class="math notranslate nohighlight">\(k\)</span> goes to infinity, this function converges to <span class="math notranslate nohighlight">\(0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.  However, its derivative diverges.
This is a problem for force fields, because the forces are given by the derivative of the potential energy with respect to the coordinates.
Consequently, we need to ensure that the forces converge as well.</p>
<p>This is the basis for the <em>force matching</em> method.
In this method, we minimize the difference between the forces predicted by our model and the forces predicted by quantum mechanics, in addition to the difference between the energies.
The loss functional is given by:</p>
<div class="math notranslate nohighlight">
\[
L(f(x, \theta), y) = ||f(x, \theta) - y||^2 + ||\sum_i (\frac{\partial}{\partial x_i} f(x, \theta) - f_{iy})||^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_{iy}\)</span> is the force predicted by quantum mechanics in coordinate <span class="math notranslate nohighlight">\(i\)</span> for the <span class="math notranslate nohighlight">\(y\)</span>th sampled configuration.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="optim-01-intro-optimization.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Optimization</p>
      </div>
    </a>
    <a class="right-next"
       href="optim-03a-autograd-in-pytorch.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Automatic Differentiation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Nonlinear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-logistic-regression">Example: Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-to-neural-networks">Extension to Neural Networks</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-density-estimation">Example: Density Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-learning-force-fields">Example: Learning Force Fields</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Thiede
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>