
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Markov Chain Monte Carlo &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture-09-mcmc';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Detailed Balance and the Metropolis-Hastings Algorithm" href="lecture-10-metropolis-hastings.html" />
    <link rel="prev" title="Monte Carlo" href="lecture-08-monte-carlo.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to 7870
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1. Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-01-intro_vectors.html">A Deeper look at Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-02-coding_matrices_in_python.html">Linear Algebra in Python with Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-03-matrix_decompositions.html">Eigenvectors, Unitary Matrices, and Matrix Decompositions</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-04-linear_regression.html">Linear Regression and Least Squares</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2. Probability</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-05-probability.html">Intro to Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-05a-random-in-numpy.html">Using Random Numbers in Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-06-bayes-rule.html">Conditional Probability and Bayes Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-07-bayesian-inference.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-08-monte-carlo.html">Monte Carlo</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-10-metropolis-hastings.html">Detailed Balance and the Metropolis-Hastings Algorithm</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3. Numerical Differential Equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-11-ode-intro.html">Intro to Numerical Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-12-symplectic-integrators.html">More on Numerical ODEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-13-pde.html">From ODE to PDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-14-optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-15-nonlinear-regression.html">Nonlinear Regression</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture-09-mcmc.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/lecture-09-mcmc.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Markov Chain Monte Carlo</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Markov Chain Monte Carlo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-using-markov-chains">Sampling using Markov Chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gibbs-sampling-and-the-ising-model">Gibbs Sampling and the Ising Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ising-model">The Ising Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="markov-chain-monte-carlo">
<h1>Markov Chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Understand the basic principles behind Markov Chain Monte Carlo (MCMC) methods.</p></li>
<li><p>Understand how Gibbs sampling works and how to apply it to the Ising model.</p></li>
</ul>
</section>
<section id="id1">
<h2>Markov Chain Monte Carlo<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>In the previous examples of Monte Carlo, we sampled from simple distributions such as the uniform distribution or the normal distribution.   But what if we have a complicated distribution?  For instance, in statistical mechanics we might wish to sample from the Boltzmann distribution, which is given by</p>
<div class="math notranslate nohighlight">
\[
\pi(x) = \frac{1}{Z} e^{-\beta H(x)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is the partition function and <span class="math notranslate nohighlight">\(H(x)\)</span> is the Hamiltonian.  In general, <span class="math notranslate nohighlight">\(H\)</span> can be a very complicated function of the system’s state <span class="math notranslate nohighlight">\(x\)</span>.  For instance, in a system of <span class="math notranslate nohighlight">\(N\)</span> particles, <span class="math notranslate nohighlight">\(x\)</span> might be a list of <span class="math notranslate nohighlight">\(3N\)</span> coordinates, and <span class="math notranslate nohighlight">\(H(x)\)</span> might be a sum of pairwise interactions between particles.</p>
<div class="math notranslate nohighlight">
\[
H(q, p) = \sum_{i=1}^N \sum_{j=i+1}^N V(q_i - q_j) + \frac{1}{2} \sum_{i=1}^N V(p_i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(V\)</span> is the interaction potential between particles and we have separated the coordinates into position coordinates <span class="math notranslate nohighlight">\(q\)</span> and momentum coordinates <span class="math notranslate nohighlight">\(p\)</span>.  If our system has <span class="math notranslate nohighlight">\(N=100\)</span> particles, then <span class="math notranslate nohighlight">\(H\)</span> is a function of <span class="math notranslate nohighlight">\(600\)</span> variables, and there is no easy way to sample from <span class="math notranslate nohighlight">\(p(x)\)</span> directly.  In another example, in Bayesian Inference, we might wish to calculate averages over the posterior distribution, which is given by Bayes’ theorem</p>
<div class="math notranslate nohighlight">
\[
p(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(D\)</span> is the data, <span class="math notranslate nohighlight">\(\theta\)</span> is the parameter, <span class="math notranslate nohighlight">\(p(D|\theta)\)</span> is the likelihood, <span class="math notranslate nohighlight">\(p(\theta)\)</span> is the prior, and <span class="math notranslate nohighlight">\(p(D)\)</span> is the evidence.
In prior examples, we have discussed attempting to sample from <span class="math notranslate nohighlight">\(p(\theta)\)</span>, and mentioned how this may not be optimal at large amounts of data, when <span class="math notranslate nohighlight">\(P(D | \theta)\)</span> is very peaked.
If we could sample from <span class="math notranslate nohighlight">\(p(\theta | D)\)</span> we could avoid this problem.  However, this can be a very complicated distribution, no less so because the evidence <span class="math notranslate nohighlight">\(p(D)\)</span>, much like the partition function <span class="math notranslate nohighlight">\(Z\)</span>, typically cannot be calculated directly.</p>
<section id="sampling-using-markov-chains">
<h3>Sampling using Markov Chains<a class="headerlink" href="#sampling-using-markov-chains" title="Link to this heading">#</a></h3>
<p>To address these problems, we give up on sampling statistically  samples.  Instead, we draw correlated samples through a technique called <code class="docutils literal notranslate"><span class="pre">Markov</span> <span class="pre">chain</span> <span class="pre">Monte</span> <span class="pre">Carlo</span></code> (MCMC).  In MCMC, we draw new samples from a distribution that is conditioned on the prior sample.
We then repeat this process until we have enough samples.  If we choos our sampling distribution correctly, averages over our samples will converge to the averages over the target distribution.  This is the basic idea behind MCMC.</p>
<p>Mathematically, we have</p>
<ol class="arabic simple">
<li><p>A distribution <span class="math notranslate nohighlight">\(\pi(x)\)</span> that we want to sample from.</p></li>
<li><p>A probability density <span class="math notranslate nohighlight">\(p(x_t | x_{t-1})\)</span> that encodes the probability of moving from <span class="math notranslate nohighlight">\(x_{t-1}\)</span> to <span class="math notranslate nohighlight">\(x_t\)</span>. We call this the transition probability.
(As always, we present our results in terms of continuous variables with probability densities, but the same ideas apply to discrete variables with probability mass functions.)</p></li>
</ol>
<p>For the algorithm to work, we require that <span class="math notranslate nohighlight">\(\pi\)</span> is the stationary distribution of the transition probability, i.e.,</p>
<div class="math notranslate nohighlight">
\[
\int p(x_t | x_{t-1}) \pi(x_{t-1}) dx_{t-1} = \pi(x_t)
\]</div>
<p>If this condition holds, if we (a) randomly draw a sample <span class="math notranslate nohighlight">\(X_0\)</span> from <span class="math notranslate nohighlight">\(\pi(x)\)</span>, and (b) draw a sample <span class="math notranslate nohighlight">\(X_1\)</span> from <span class="math notranslate nohighlight">\(p(X_1 | X_0)\)</span>, and (c) draw a sample <span class="math notranslate nohighlight">\(X_2\)</span> from <span class="math notranslate nohighlight">\(p(X_2 | X_1)\)</span>, and so on, then each sample <span class="math notranslate nohighlight">\(X_0, X_1, X_2, \ldots\)</span> will be individually distributed according to <span class="math notranslate nohighlight">\(\pi(x)\)</span>.  The sequence of random variables <span class="math notranslate nohighlight">\(X_0, X_1, X_2, \ldots\)</span> is called a Markov chain.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Having <span class="math notranslate nohighlight">\(\pi\)</span> as a stationary distribution of the transition probability is a necessary condition for the algorithm to work, but it is not sufficient.  The transition probability must also be ergodic, meaning that it is possible to get from any state to any other state in a finite number of steps, and be aperiodic, meaning that the chain does not get stuck in a loop.  These are, however, much more difficult conditions to check.</p>
</div>
</section>
</section>
<section id="gibbs-sampling-and-the-ising-model">
<h2>Gibbs Sampling and the Ising Model<a class="headerlink" href="#gibbs-sampling-and-the-ising-model" title="Link to this heading">#</a></h2>
<p>One simple Markov  chain Monte Carlo example is  Gibbs sampling.  In Gibbs sampling, at each step we grab a single coordinate from our description of the system and update it by sampling from the conditional distribution of that coordinate given all the others.
Specifically, we update <span class="math notranslate nohighlight">\(x^k\)</span>, the <span class="math notranslate nohighlight">\(k\)</span>’th coordinate of <span class="math notranslate nohighlight">\(x\)</span>, by sampling the new value from the conditional distribution</p>
<div class="math notranslate nohighlight">
\[
p(x^k | x^1, \ldots, x^{k-1},  x^{k+1}, \ldots, x^N) = \frac{\pi(x^1, \ldots, x^{k-1}, x^k, x^{k+1}, \ldots, x^N)}{\int \pi(x^1, \ldots, x^N) dx^1, \ldots, dx^{k-1}, dx^{k+1}, \ldots, dx^N}
\]</div>
<p>We will show that this process preserves the distribution <span class="math notranslate nohighlight">\(\pi(x)\)</span> later, in our discussion of the Metroplis-Hastings algorithm.
For now, let’s consider a specific example of Gibbs sampling.</p>
<!-- Writing this out every time is going to be tiring, so we will adopt a shorthand notation where we write $x^{i \neq k}$ to denote all of the coordinates except for $x^k$.  In this notation, we can write the conditional distribution as

$$
p(x^k | x^{i \neq k}) = \frac{\pi(x^k, x^{i \neq k})}{\int \pi(y^k, x^{i \neq k})  dy^{ k}}.
$$ -->
<section id="the-ising-model">
<h3>The Ising Model<a class="headerlink" href="#the-ising-model" title="Link to this heading">#</a></h3>
<p>The Ising model is a simple model of a magnet.  In the Ising model, we have a lattice of spins, each of which can be either up or down.  The energy of the system is given by</p>
<div class="math notranslate nohighlight">
\[
H(s_1, \ldots, s_M) = -J/2 \sum_{j} \sum_{i\in N_j } s_i s_j
\]</div>
<p>where the sum is over nearest neighbors (<span class="math notranslate nohighlight">\(N_j\)</span> are all of the spins neighboring spin <span class="math notranslate nohighlight">\(j\)</span>), <span class="math notranslate nohighlight">\(s_i\)</span> is the spin at site <span class="math notranslate nohighlight">\(i\)</span> and takes a value of <span class="math notranslate nohighlight">\(+1\)</span> for up and <span class="math notranslate nohighlight">\(-1\)</span> for down, and <span class="math notranslate nohighlight">\(J\)</span> is the coupling constant.  The probability of seeing a collection of spins up or down is given by the Boltzmann distribution</p>
<div class="math notranslate nohighlight">
\[
\pi(s_1, \ldots s_M) = \frac{1}{Z} e^{-\beta H(s_1, \ldots, s_M)}
\]</div>
<p>where <span class="math notranslate nohighlight">\(Z\)</span> is the partition function.</p>
<!-- In Gibbs sampling, we randomly pick a single spin (denoted $s_k$) and update it according to the conditional distribution -->
<p>To apply Gibbs sampling to the Ising model, we randomly pick a single spin (denoted <span class="math notranslate nohighlight">\(s_k\)</span>).
The conditional distribution of that spin given all the others has the probability mass function</p>
<div class="math notranslate nohighlight">
\[
p(s_k | s_{i \neq k}) = \frac{e^{-\beta H(s_1, \ldots, s_k, \ldots, S_m )  } }{
    e^{-\beta H(s_1, \ldots, +1, \ldots, S_m )}   + 
    e^{-\beta H(s_1, \ldots, -1, \ldots, S_m )}
}
= 
\frac{e^{\beta J \sum_{i \in N_k} s_i} }{
    e^{\beta J \sum_{i \in N_k} s_i}   + 
    e^{\beta -J \sum_{i \in N_k} s_i}
}
\]</div>
<p>This is gives us a simple algorithm for sampling from the Ising Model:</p>
<ol class="arabic simple">
<li><p>We pick a random spin <span class="math notranslate nohighlight">\(s_k\)</span>.</p></li>
<li><p>We calculate the conditional probability of that spin being up or down.</p></li>
<li><p>We sample from that conditional probability to get the new value of <span class="math notranslate nohighlight">\(s_k\)</span>.</p></li>
<li><p>We repeat steps 1-3 until we have enough samples.</p></li>
</ol>
<p>Note that this conditional probability is a function of only the spins neighboring <span class="math notranslate nohighlight">\(s_k\)</span>.  This means that we can update spins in parallel as long as they are not neighbors.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>If you want an example of how to implement Gibbs sampling on a 2D normal distribution where we alternate between sampling from the conditional distribution of <span class="math notranslate nohighlight">\(x\)</span> given <span class="math notranslate nohighlight">\(y\)</span> and <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>, see <a class="reference external" href="https://mr-easy.github.io/2020-05-21-implementing-gibbs-sampling-in-python/">this blog post</a></p></li>
<li><p>This recorded <a class="reference external" href="https://www.youtube.com/watch?v=vTUwEu53uzs">video lecture</a> gives an introduction to Markov chain Monte Carlo and its applications for Bayesian Inference in Statistics.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture-08-monte-carlo.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Monte Carlo</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture-10-metropolis-hastings.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Detailed Balance and the Metropolis-Hastings Algorithm</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Markov Chain Monte Carlo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-using-markov-chains">Sampling using Markov Chains</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gibbs-sampling-and-the-ising-model">Gibbs Sampling and the Ising Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-ising-model">The Ising Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Thiede
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>