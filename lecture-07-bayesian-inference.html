
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Bayesian Inference &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture-07-bayesian-inference';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Monte Carlo" href="lecture-08-monte-carlo.html" />
    <link rel="prev" title="Conditional Probability and Bayes Rule" href="lecture-06-bayes-rule.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to 7870
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1. Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-01-intro_vectors.html">A Deeper look at Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-02-coding_matrices_in_python.html">Linear Algebra in Python with Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-03-matrix_decompositions.html">Eigenvectors, Unitary Matrices, and Matrix Decompositions</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-04-linear_regression.html">Linear Regression and Least Squares</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2. Probability</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-05-probability.html">Intro to Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-05a-random-in-numpy.html">Using Random Numbers in Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-06-bayes-rule.html">Conditional Probability and Bayes Rule</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-08-monte-carlo.html">Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-09-mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-10-metropolis-hastings.html">Detailed Balance and the Metropolis-Hastings Algorithm</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3. Numerical Differential Equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-11-ode-intro.html">Intro to Numerical Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-12-symplectic-integrators.html">More on Numerical ODEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-13-pde.html">From ODE to PDE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture-07-bayesian-inference.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/lecture-07-bayesian-inference.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Bayesian Inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior">Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence">Evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-multiple-data-points">Working with multiple data points</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-in-log-space">Working in log-space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="bayesian-inference">
<h1>Bayesian Inference<a class="headerlink" href="#bayesian-inference" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Develop a sense for constructing Priors and likelihoods</p></li>
<li><p>Understand how to evaluate the evidence (and when you might not want to).</p></li>
<li><p>Understand how to use the posterior to make predictions.</p></li>
<li><p>Become familiar with working in log-space for numerical stability.</p></li>
<li><p>Understand how to apply Bayesian Inference to linear regression.</p></li>
</ul>
</section>
<section id="id1">
<h2>Bayesian Inference<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>In our previous discussion of Bayesian Inference, we advanced an approach for evaluating statistical hypotheses.  Given a set of data, <span class="math notranslate nohighlight">\(D\)</span>, and hypothesis for the model that generated the data, <span class="math notranslate nohighlight">\(H\)</span>, we can evaluate the posteriar probability of the hypothesis given the data, <span class="math notranslate nohighlight">\(P(H|D)\)</span> using Bayes Rule:</p>
<div class="math notranslate nohighlight">
\[
P(H|D) = \frac{P(D|H)P(H)}{P(D)}.
\]</div>
<p>To recap, the terms in this equation are as follows.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P(H|D)\)</span> is the posterior probability of the hypothesis given the data.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(D|H)\)</span> is the likelihood of the data given the hypothesis, which represents the probability of observing the data given that the hypothesis is true.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(H)\)</span> is the prior probability of the hypothesis, which represents our belief in the hypothesis before seeing the data.</p></li>
<li><p><span class="math notranslate nohighlight">\(P(D)\)</span> is the evidence, or the probability of the data.</p></li>
</ul>
<p>We first discuss the construction of the prior, before moving on to the likelihood and finally the evidence.</p>
<section id="prior">
<h3>Prior<a class="headerlink" href="#prior" title="Link to this heading">#</a></h3>
<p>The prior is the probability of the hypothesis before seeing the data: typically this is something that you choose based on your knowledge of the system.  In general, the hypothesis is encoded in one or more parameters that specify the hypothesis.</p>
<p>For example, if you are trying to determine the probability that a coin comes up heads you might choose a prior for the probability of heads.  Some examples of priors are:</p>
<ul class="simple">
<li><p>If you have no idea what the probability is, you might choose a prior that is uniform over the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>.</p></li>
<li><p>If you have good reason to believe that the coin is fair, you might choose a prior that is peaked at <span class="math notranslate nohighlight">\(0.5\)</span>.</p></li>
<li><p>If say someone came to you with a glimmer in their eye and said “Want to place a bet that this coin will come up heads?” you probably have a good reason to believe they are trying to cheat you and might choose a prior that has larger values at very low probabilities!</p></li>
</ul>
<p>Priors can also encode experimental constraints.  For example, say you are determining the time it takes for a radioactive isotope to decay.  In this case, our hypothesis is specified by the decay constant..  You might not have a good idea of what the decay constant is, but you do know that it must be positive.  In this case, you might choose a prior that is uniform over the interval <span class="math notranslate nohighlight">\([0, \infty)\)</span>, but zero from <span class="math notranslate nohighlight">\(-\infty\)</span> to <span class="math notranslate nohighlight">\(0\)</span>.</p>
<p>Typically, we call uniform, or very flat priors “uninformative:” in most cases, we use them when we have no information about the system.
However, if we have strong beliefs about the system we might use an “informative” prior, which prefers certain values of the hypothesis over others.</p>
</section>
<section id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Link to this heading">#</a></h3>
<p>The likelihood is the probability of observing the data given the hypothesis.  Constructing the likelihood is often the most challenging part of Bayesian Inference, and often involves explicitly modelling any noise or randomness in the data.  For example, if you are measuring the time it takes for a radioactive isotope to decay, you might model the decay time as a random variable with an exponential distribution.  The likelihood is then the probability of observing the data given the decay time.  We can associate this likelihood with the probability density function of the random variable,</p>
<div class="math notranslate nohighlight">
\[
p(x | \lambda) = \lambda e^{-\lambda x},
\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the observed decay time, and <span class="math notranslate nohighlight">\(\lambda\)</span> is the rate parameter of the exponential distribution.  The likelihood is then the probability of observing the data given the rate parameter.  In general, it is common to also refer to the probability density or mass function as the likelihood as well, and just have which one we are talking about be clear from context.  Hereafter we will refer to the probability density, and trust the reader to generalize to the probability mass function.</p>
<p>Another common example is the Gaussian likelihood, where we assume the observed data comes from a Gaussian distribution with mean <span class="math notranslate nohighlight">\(\mu\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>.  The probability density for the likelihood is given by.</p>
<div class="math notranslate nohighlight">
\[
p(y | \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y - \mu)^2}{2\sigma^2}}.
\]</div>
</section>
<section id="evidence">
<h3>Evidence<a class="headerlink" href="#evidence" title="Link to this heading">#</a></h3>
<p>The final piece needed for Bayes Rule is the evidence, or the probability of the data.  This is often the most challenging part of Bayesian Inference, as it involves integrating over all possible hypotheses.  Fortunately, in many cases, we might not need it:</p>
<ul class="simple">
<li><p>If we only want to find the most probable hypothesis (aka the <em>maximum a posteriori</em> or MAP estimate), we can ignore the evidence, as it is a constant that does not depend on the hypothesis.</p></li>
<li><p>If we want to compare the relative probabilities of two hypotheses, we can calculate the ratio of the posteriors, called the <em>Bayes factor</em>, given in the equation below.  In this case, the evidence cancels out.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{Bayes Factor} = \frac{P(H_1|D)}{P(H_2|D)} = \frac{P(D|H_1)P(H_1)}{P(D|H_2)P(H_2)}.
\]</div>
<p>However, in other cases we might need to calculate the evidence, or at least the probability density or probability mass function associated.<br />
Denote the possible value for the random variable as <span class="math notranslate nohighlight">\(x\)</span> and the parameter specifying the hypothesis as <span class="math notranslate nohighlight">\(\theta\)</span>.<br />
For example, if we want to calculate expected values over the posterior:</p>
<ul class="simple">
<li><p>The expected value for a given  parameter is given by <span class="math notranslate nohighlight">\(\int \theta p(\theta|x) d\theta\)</span>.</p></li>
<li><p>Similarly, its variance is given by <span class="math notranslate nohighlight">\(\int \theta^2 p(\theta|x) d\theta - \left(\int \theta p(\theta|x) d\theta\right)^2\)</span>.
In both of these cases we need to know the probability density for the posterior.
From Bayes Rule, we expect that</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(\theta | x ) \propto {p(x | \theta) p(\theta)}.
\]</div>
<p>To be able to take expectations over <span class="math notranslate nohighlight">\(\theta\)</span>, we need to normalize this distribution.  This requires calculating <span class="math notranslate nohighlight">\(\int p(x | \theta) p(\theta) d\theta\)</span>.  Using the law of total probability, we see that this is <span class="math notranslate nohighlight">\(p(x)\)</span>: the the probability density associated with the evidence.  In general, calculating this integral is difficult.  For now, we will focus on the case where we can ignore the evidence, or when we can approximate this integral using quadrature.</p>
</section>
</section>
<section id="working-with-multiple-data-points">
<h2>Working with multiple data points<a class="headerlink" href="#working-with-multiple-data-points" title="Link to this heading">#</a></h2>
<p>If we have multiple data points, <span class="math notranslate nohighlight">\(D = \{y_1, y_2, \ldots, y_n\}\)</span>, and we assume that the data points are independent and identically distributed (i.i.d.), we can build a new likelihood using the fact that probabilities of independent events multiply.  In this case, the likelihood is the product of the likelihoods of the individual data points:</p>
<div class="math notranslate nohighlight">
\[
p(y_1, \ldots, y_n | \theta) = \prod_{i=1}^n p(y_i | \theta),
\]</div>
<p>This leads to a problem: we are multiplying many small numbers together, which can quickly become numerically unstable (i.e., the numbers become too small to be represented by the computer).</p>
<section id="working-in-log-space">
<h3>Working in log-space<a class="headerlink" href="#working-in-log-space" title="Link to this heading">#</a></h3>
<p>To avoid this, we can work in log-space.  Rather than taking the product of the likelihoods, we take the sum of the log-likelihoods:</p>
<div class="math notranslate nohighlight">
\[
\log p(y_1, \ldots, y_n | \theta) = \sum_{i=1}^n \log p(y_i | \theta).
\]</div>
<p>Note that the prior then becomes an additive term in the log-likelihood, and the evidence becomes an additive constant:</p>
<div class="math notranslate nohighlight">
\[
\log p(\theta | y_1, \ldots, y_n) = \sum_{i=1}^n \log p(y_i | \theta).  + \log p(\theta)  + \text{constant}.
\]</div>
<p>This term also shows us how to balance the relative importance of the prior and the likelihood.  At small amounts of data, the prior can have a considerable effect.  However, as we keep adding data, the likelihood term will dominate.  This is a key feature of Bayesian Inference: the more data you have, the more the data will drive the posterior probability and the less the prior will matter.</p>
</section>
</section>
<section id="bayesian-linear-regression">
<h2>Bayesian Linear Regression<a class="headerlink" href="#bayesian-linear-regression" title="Link to this heading">#</a></h2>
<p>The true power of Bayesian Inference comes from its ability to handle complex models.  For example, consider the case of linear regression.  In linear regression, we assume that the data points <span class="math notranslate nohighlight">\(y_i\)</span> are generated by a linear model from some known quantity <span class="math notranslate nohighlight">\(x_i\)</span> with unknown slope <span class="math notranslate nohighlight">\(m\)</span> and intercept <span class="math notranslate nohighlight">\(b\)</span>.  In this case, the likelihood is given by</p>
<div class="math notranslate nohighlight">
\[
p(y_1, \ldots, y_n | m, b) = \prod_{i=1}^n p(y_i | m, b, x_i),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(y_i | m, b, x_i)\)</span> is the probability density of the Gaussian distribution with mean <span class="math notranslate nohighlight">\(m x_i + b\)</span> and standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>,</p>
<div class="math notranslate nohighlight">
\[
p(y_i | m, b, x_i) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(y_i - m x_i - b)^2}{2\sigma^2}}.
\]</div>
<p>In this case, the hypothesis is specified by the parameters <span class="math notranslate nohighlight">\(m\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.  We can then use Bayes Rule to calculate the posterior probability of the parameters given the data.  This is a powerful tool, as it allows us to quantify the uncertainty in the parameters of the model.  For example, we can calculate the probability that the slope is positive, or the probability that the slope is within some range.</p>
<p>If we have a large amount of data, our model is dominated by the likelihood term.  Seeking the maximum a posteriori estimate is equivalent to maximizing the log-likelihood, which is in turn given by</p>
<div class="math notranslate nohighlight">
\[
\log p(y_1, \ldots, y_n | m, b) = \sum_{i=1}^n \log p(y_i | m, b, x_i) = -\frac{1}{2\sigma^2} \sum_{i=1}^n (y_i - m x_i - b)^2 + \text{constant}.
\]</div>
<p>This is precisely the same as the least squares estimate for linear regression.  As such, our Bayesian estimate generalizes the least squares estimate, and provides a principled way to quantify its uncertainty.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://www.statlect.com/fundamentals-of-statistics/Bayesian-inference">This Link</a> contains much of the information in this lecture, presented in a slightly different way.</p></li>
<li><p>I found a video on <a class="reference external" href="https://www.youtube.com/watch?v=3jP4H0kjtng">Bayesian Statistics</a> that seems good, but I haven’t watched it closely.  It does seem very meme heavy, so be prepared for that.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture-06-bayes-rule.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Conditional Probability and Bayes Rule</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture-08-monte-carlo.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Monte Carlo</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Bayesian Inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prior">Prior</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#likelihood">Likelihood</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evidence">Evidence</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#working-with-multiple-data-points">Working with multiple data points</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#working-in-log-space">Working in log-space</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-linear-regression">Bayesian Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Thiede
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>