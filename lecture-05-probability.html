
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Intro to Probability &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture-05-probability';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Using Random Numbers in Numpy" href="lecture-05a-random-in-numpy.html" />
    <link rel="prev" title="Linear Regression and Least Squares" href="lecture-04-linear_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to 7870
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1. Linear Algebra</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-01-intro_vectors.html">A Deeper look at Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-02-coding_matrices_in_python.html">Linear Algebra in Python with Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-03-matrix_decompositions.html">Eigenvectors, Unitary Matrices, and Matrix Decompositions</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-04-linear_regression.html">Linear Regression and Least Squares</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 2. Probability</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Intro to Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-05a-random-in-numpy.html">Using Random Numbers in Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-06-bayes-rule.html">Conditional Probability and Bayes Rule</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-07-bayesian-inference.html">Bayesian Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-08-monte-carlo.html">Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-09-mcmc.html">Markov Chain Monte Carlo</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-10-metropolis-hastings.html">Detailed Balance and the Metropolis-Hastings Algorithm</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 3. Numerical Differential Equations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-11-ode-intro.html">Intro to Numerical Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-12-symplectic-integrators.html">More on Numerical ODEs</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-13-pde.html">From ODE to PDE</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-14-optimization.html">Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-15-nonlinear-regression.html">Nonlinear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-16-autograd.html">Automatic Differentiation and JaX</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture-05-probability.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/lecture-05-probability.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Intro to Probability</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-and-chemistry">Probability and Chemistry</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-non-rigorous-introduction-to-probability">A (Non-Rigorous) Introduction to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-random-variable">What is a Random Variable?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-probabilities">Quantifying Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmfs">Probability Mass Functions (PMFs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdfs">Probability Density Functions (PDFs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#changing-variables-changes-the-pdf">Changing Variables Changes the PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-probability-distribution">Sampling from a Probability Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doing-things-with-random-variables">Doing things with Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-values">Expected Values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-with-free-energy">Connection with Free Energy</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="intro-to-probability">
<h1>Intro to Probability<a class="headerlink" href="#intro-to-probability" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Understand the basic concepts of probability theory and what a probability space is.</p></li>
<li><p>Understand what a Random Variable is and how it is used to describe random phenomena.</p></li>
<li><p>Be proficient in using and manipulating probability mass and density functions.</p></li>
<li><p>Connect probability theory to the concept of free energy in statistical mechanics.</p></li>
</ul>
</section>
<section id="probability-and-chemistry">
<h2>Probability and Chemistry<a class="headerlink" href="#probability-and-chemistry" title="Link to this heading">#</a></h2>
<p>Randomness is everywhere in chemistry.  Just to name a few examples:</p>
<ul class="simple">
<li><p>Microscopic systems are randomly kicked by thermal motion, causing fluctations in their state.</p></li>
<li><p>Quantum mechanics is inherently probabilistic: when we measure a quantum system, it randomly collapses to one of the possible states.</p></li>
<li><p>Measurements are subject due to random errors, and we can only estimate the true value of a quantity with some uncertainty.
Probability is the mathematical framework that allows us to reason about these random phenomena.</p></li>
</ul>
<p>There are two main interpretations of probability:</p>
<ul class="simple">
<li><p><strong>Frequentist</strong>: Probability is the long-run frequency of an event occurring.</p></li>
<li><p><strong>Bayesian</strong>: Probability is a measure of the degree of belief that an event will occur.
These interpretations do not change the mathematical rules of probability, but they do change what we might find interesting or useful to calculate.
For now, we will focus on how probability is constructed, and come back to the difference in these interpretations later.</p></li>
</ul>
<section id="a-non-rigorous-introduction-to-probability">
<h3>A (Non-Rigorous) Introduction to Probability<a class="headerlink" href="#a-non-rigorous-introduction-to-probability" title="Link to this heading">#</a></h3>
<p>Mathematicians describe probability using the following terms.
For any random event, we can define a probability space <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>, where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Omega\)</span> is the set of all possible outcomes of the event.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a set of ``events’’: all the subsets of reasonable outcomes that we might want to assign a probability to.  (Formally, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a <span class="math notranslate nohighlight">\(\sigma\)</span>-algebra of subsets of <span class="math notranslate nohighlight">\(\Omega\)</span>.  This is far more technical than we need to get into here.)</p></li>
<li><p><span class="math notranslate nohighlight">\(P\)</span> is a probability measure, which assigns a number between 0 and 1 to each event in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, such that <span class="math notranslate nohighlight">\(P(\Omega) = 1\)</span> and <span class="math notranslate nohighlight">\(P(\emptyset) = 0\)</span>.</p></li>
</ul>
<section id="examples">
<h4>Examples:<a class="headerlink" href="#examples" title="Link to this heading">#</a></h4>
<p>Consider rolling a six-sided, evenly weighted die.  The probability space is <span class="math notranslate nohighlight">\((\Omega, \mathcal{F}, P)\)</span>, where:</p>
<ul class="simple">
<li><p>Each outcome in <span class="math notranslate nohighlight">\(\Omega\)</span> is which face of the die comes up.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is the set of outcomes: for instance, if the face is a one or two.</p></li>
</ul>
<!-- - $\Omega = \{1, 2, 3, 4, 5, 6\}$. -->
<!-- - $\mathcal{F} = \{\emptyset, \{1\}, \{2\}, \{3\}, \{4\}, \{5\}, \{6\}, \{1, 2\}, \{1, 3\}, \ldots, \{1, 2, 3, 4, 5, 6\}\}$. -->
<!-- - $P(\{1\}) = P(\{2\}) = \ldots = P(\{6\}) = 1/6$.  Similarly, $P(\{1, 2\}) = P(\{1, 3\}) = \ldots = 1/3$, and so on. -->
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(P\)</span> gives the probability of each event in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.  For example, the probability of rolling a one is <span class="math notranslate nohighlight">\(1/6\)</span>, or the probability of rolling an even number is <span class="math notranslate nohighlight">\(1/2\)</span>.</p></li>
</ul>
<p>Another example is in equilibrium statistical mechanics,</p>
<ul class="simple">
<li><p>Microstates are the possible outcomes of a random event, and are elements in <span class="math notranslate nohighlight">\(\Omega\)</span>.</p></li>
<li><p>Macrostates, which are sets of possible microstates, are elements in <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>.</p></li>
<li><p>The Boltzmann distribution is a specific example of a probability measure, which assigns a probability to each macrostate.</p></li>
</ul>
</section>
</section>
<section id="what-is-a-random-variable">
<h3>What is a Random Variable?<a class="headerlink" href="#what-is-a-random-variable" title="Link to this heading">#</a></h3>
<p>A <strong>random variable</strong> is a function that maps outcomes of a random event to real numbers.</p>
<!-- It is *not* the same thing as an element in $\Omega$: observing  -->
<p>Care must be taken to distinguish between random variables and elements in <span class="math notranslate nohighlight">\(\Omega\)</span>.  Coming back to the die example, the random variable <span class="math notranslate nohighlight">\(X\)</span> might be the number that comes up when the die is rolled.  <span class="math notranslate nohighlight">\(X\)</span> is a function that maps the event “a one comes up” to the number 1, and so on.  However, these are not the same thing: one is a physical event, whereas the other is a mathematical object.  Additionally, there are many other possible random variables we can define.  For instance, another random variable would be the parity of the number that comes up: 0 if the number is even, and 1 if the number is odd.  Yet another would be the square of the number on the face.</p>
<p>Note that since random variables map to numbers, there is a natural way to add them together, multiply them, and so on.  For instance, if we have two dice, <span class="math notranslate nohighlight">\(X_1\)</span> is the random variable corresponding to the number on the first die, and <span class="math notranslate nohighlight">\(X_2\)</span> is the random variable corresponding to the number on the second die, then <span class="math notranslate nohighlight">\(X_1 + X_2\)</span> is a new random variable corresponding to the sum of the two dice.</p>
<p>We can trivially extend random variables to vector-valued functions.  For instance, if we have a die and a coin, we could define a random variable <span class="math notranslate nohighlight">\(X = (X_1, X_2)\)</span>, where <span class="math notranslate nohighlight">\(X_1\)</span> is the number on the die and <span class="math notranslate nohighlight">\(X_2\)</span> is the result of the coin flip.</p>
</section>
</section>
<section id="quantifying-probabilities">
<h2>Quantifying Probabilities<a class="headerlink" href="#quantifying-probabilities" title="Link to this heading">#</a></h2>
<p>One of the most common ways of quantifying probabilities is to use a <strong>probability mass function</strong> (PMF) for discrete random variables, or a <strong>probability density function</strong> (PDF) for continuous random variables.</p>
<section id="probability-distributions">
<h3>Probability Distributions<a class="headerlink" href="#probability-distributions" title="Link to this heading">#</a></h3>
<p>Random variables are assigned probabilities using a <strong>probability distribution</strong>.  This is a function that assigns probabilities to the possible values of the random variable.  Again, consider the case where <span class="math notranslate nohighlight">\(X\)</span> is the number on a six-sided die.  The probability distribution of <span class="math notranslate nohighlight">\(X\)</span> assigns a probability of <span class="math notranslate nohighlight">\(1/6\)</span> to the numbers 1, 2, 3, 4, 5, and 6.  Similarly, the probability of seeing <span class="math notranslate nohighlight">\(X = 1\)</span> or <span class="math notranslate nohighlight">\(X = 2\)</span> is <span class="math notranslate nohighlight">\(1/3\)</span>, and so on.</p>
<p>To evaluate probability distributions, we often use two additional functions: “probability mass functions” (PMFs) for discrete random variables, and “probability density functions” (PDFs) for continuous random variables.</p>
</section>
<section id="probability-mass-functions-pmfs">
<h3>Probability Mass Functions (PMFs)<a class="headerlink" href="#probability-mass-functions-pmfs" title="Link to this heading">#</a></h3>
<p>A PMF is a function <span class="math notranslate nohighlight">\(p(x)\)</span> that gives the probability that a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> takes on a specific value <span class="math notranslate nohighlight">\(x\)</span>.  The PMF must satisfy two properties:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(0 \leq p(x) \leq 1\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\sum_x p(x) = 1\)</span>.
For example, the probability mass function the number seen on a six-sided die is <span class="math notranslate nohighlight">\(p(x) = 1/6\)</span> for <span class="math notranslate nohighlight">\(x = 1, 2, 3, 4, 5, 6\)</span>.
The probabilities of events can be calculated by summing the probabilities of the outcomes in the event.
For example, the probability of rolling an even number is <span class="math notranslate nohighlight">\(p(\{2, 4, 6\}) = p(2) + p(4) + p(6) = 1/6 + 1/6 + 1/6 = 1/2\)</span>.</p></li>
</ol>
<p>Importantly, note that summing a PMF is <strong>NOT</strong> the same same as summing the random variable.  Again, consider the sum of two dice.<br />
The probability of the sum being 2 is <span class="math notranslate nohighlight">\(p(2) = 1/36\)</span>, the probability of the sum being 3 is <span class="math notranslate nohighlight">\(p(3) = 2/36\)</span>, and so on.  This is clearly not the same as the PMFs for the individual rolls.</p>
</section>
<section id="probability-density-functions-pdfs">
<h3>Probability Density Functions (PDFs)<a class="headerlink" href="#probability-density-functions-pdfs" title="Link to this heading">#</a></h3>
<p>Probability density functions are the continuous analog of probability mass functions.  A PDF <span class="math notranslate nohighlight">\(p(x)\)</span> gives the probability that a continuous random variable <span class="math notranslate nohighlight">\(X\)</span> takes on a value in the interval <span class="math notranslate nohighlight">\([a, b]\)</span> as <span class="math notranslate nohighlight">\(\int_a^b p(x) dx\)</span>.  The PDF must satisfy two properties:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(p(x) \geq 0\)</span> for all <span class="math notranslate nohighlight">\(x\)</span>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\int_{-\infty}^\infty p(x) dx = 1\)</span>.
Note that, unlike the PMF case, <span class="math notranslate nohighlight">\(p(x)\)</span> can be greater than 1 for some values of <span class="math notranslate nohighlight">\(x\)</span>.
Moreover, the probability of a continuous random variable taking on a specific value is zero: after all, <span class="math notranslate nohighlight">\(\int_a^a p(x) dx = 0\)</span>.
The PDF doesn’t give probabilities, it gives a <em>density</em> of probability.  Consequently, the units of the PDF are probability per unit length / area / volume / etc, depending on the dimensionality of the random variable.</p></li>
</ol>
<p>A classic example of a PDF is the “bell curve” or Gaussian probability density.  For a one-dimensional random variable, this is given by</p>
<div class="math notranslate nohighlight">
\[
p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right),
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean of the distribution and <span class="math notranslate nohighlight">\(\sigma\)</span> is the standard deviation.  The PDF is centered at <span class="math notranslate nohighlight">\(\mu\)</span> and has a width of <span class="math notranslate nohighlight">\(\sigma\)</span>.  The integral of the PDF over all space is 1, as required.</p>
<p>Note that the PDF (or PMF) is not the same thing as probability distribution: rather, they are tools we use to describe probability distributions.
For instance, we might say that a random variable <span class="math notranslate nohighlight">\(X\)</span> obeys the Normal, or Gaussian, distribution.  In this case, we expect the probability density function to be a bell curve, as above.  Integrating over the PDF evaluates the probabilities specified by the distribution, e.g.</p>
<div class="math notranslate nohighlight">
\[
P([0, \infty)) = \int_0^\infty \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x - \mu)^2}{2\sigma^2}\right) dx = \frac{1}{2}.
\]</div>
</section>
<section id="changing-variables-changes-the-pdf">
<h3>Changing Variables Changes the PDF<a class="headerlink" href="#changing-variables-changes-the-pdf" title="Link to this heading">#</a></h3>
<p>One of the most important properties of probability density functions is that they change under a change of variables.  For instance, consider a random variable <span class="math notranslate nohighlight">\(X\)</span> with PDF <span class="math notranslate nohighlight">\(p(x)\)</span> for a real number <span class="math notranslate nohighlight">\(x\)</span>.  Let <span class="math notranslate nohighlight">\(Y = f(X)\)</span> be a new random variable, where <span class="math notranslate nohighlight">\(f\)</span> is an monotonic (always increasing or always decreasing) and differentiable function.  The PDF of <span class="math notranslate nohighlight">\(Y\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
p_Y(y) = p_X(f^{-1}(y)) \left|\frac{d}{dy} f^{-1}(y)\right|,
\]</div>
<p>where <span class="math notranslate nohighlight">\(f^{-1}(y)\)</span> is the inverse function of <span class="math notranslate nohighlight">\(f\)</span>.  This is a consequence of the fact that the probability of <span class="math notranslate nohighlight">\(Y\)</span> being in an interval <span class="math notranslate nohighlight">\([a, b]\)</span> is the same as the probability of <span class="math notranslate nohighlight">\(X\)</span> being in the interval <span class="math notranslate nohighlight">\([f^{-1}(a), f^{-1}(b)]\)</span>.</p>
</section>
<section id="sampling-from-a-probability-distribution">
<h3>Sampling from a Probability Distribution<a class="headerlink" href="#sampling-from-a-probability-distribution" title="Link to this heading">#</a></h3>
<p>Given a probability mass or density function, how might we draw samples from it?  This is a fundamental, but also extremely nontrivial, question: finding ways to efficiently sample random variables
is a going to be a major theme in our immediate future.  For now, we will focus on the simplest case: sampling from a distribution with a known probability mass function.
We assume that we have access to a random number generator that can produce random numbers uniformly distributed between 0 and 1.  Then, we can sample from a discrete distribution as follows:</p>
<ol class="arabic simple">
<li><p>Generate a random number <span class="math notranslate nohighlight">\(r\)</span> between 0 and 1.</p></li>
<li><p>For each possible value of the random variable, sum the probability of that value to a running total.  When the running total exceeds <span class="math notranslate nohighlight">\(r\)</span>, return that value.</p></li>
</ol>
<p>For instance, consider the die example.  We can generate a random number <span class="math notranslate nohighlight">\(r\)</span> between 0 and 1, and then return 1 if <span class="math notranslate nohighlight">\(r &lt; 1/6\)</span>, 2 if <span class="math notranslate nohighlight">\(1/6 \leq r &lt; 2/6\)</span>, and so on.</p>
<p>Graphically, what we have done is built a “cumulative distribution function”, which is the sum of the probability mass function up to a certain value.  We can generalize this to continuous random variables as well.  For a given probability density, we can integrate the PDF up to a certain value to get the cumulative distribution function.</p>
<div class="math notranslate nohighlight">
\[
F(x) = \int_{-\infty}^x p(x') dx'.
\]</div>
<p>Note that this is a monotonically increasing function that goes from 0 to 1.
To sample from the distribution by generating a random number <span class="math notranslate nohighlight">\(r\)</span> between 0 and 1, and returning the value of <span class="math notranslate nohighlight">\(x\)</span> such that <span class="math notranslate nohighlight">\(F(x) = r\)</span>.  This is known as the “inverse transform method”.</p>
</section>
</section>
<section id="doing-things-with-random-variables">
<h2>Doing things with Random Variables<a class="headerlink" href="#doing-things-with-random-variables" title="Link to this heading">#</a></h2>
<section id="expected-values">
<h3>Expected Values<a class="headerlink" href="#expected-values" title="Link to this heading">#</a></h3>
<p>Expected values are a way of summarizing the distribution of a random variable.  The expected value of a random variable <span class="math notranslate nohighlight">\(X\)</span> is denoted <span class="math notranslate nohighlight">\(\langle X \rangle\)</span> or <span class="math notranslate nohighlight">\(E[X]\)</span>, and is defined as the weighted average of the possible values of <span class="math notranslate nohighlight">\(X\)</span>, weighted by their probabilities.  For a discrete random variable with PMF <span class="math notranslate nohighlight">\(p(x)\)</span>, the expected value is given by <span class="math notranslate nohighlight">\(\langle X \rangle = \sum_x x p(x)\)</span>.  For a continuous random variable with PDF <span class="math notranslate nohighlight">\(p(x)\)</span>, the expected value is given by <span class="math notranslate nohighlight">\(\langle X \rangle = \int x p(x) dx\)</span>.</p>
<p>We can also take the expected value of functions of random variables.  For instance, the expected value of <span class="math notranslate nohighlight">\(X^2\)</span> is known as the second moment of <span class="math notranslate nohighlight">\(X\)</span>, and is denoted <span class="math notranslate nohighlight">\(\langle X^2 \rangle\)</span> or <span class="math notranslate nohighlight">\(E[X^2]\)</span>.  If <span class="math notranslate nohighlight">\(X\)</span> is discete and we have a PMF the variance is given by <span class="math notranslate nohighlight">\(\langle X^2 \rangle = \sum_x x^2 p(x)\)</span>.
Similarly, if <span class="math notranslate nohighlight">\(X\)</span> is continuous and we have a probability density, the variance is given by <span class="math notranslate nohighlight">\(\langle X^2 \rangle = \int x^2 p(x) dx\)</span>.  For an arbitrary function <span class="math notranslate nohighlight">\(f(X)\)</span>, the expected value is given by <span class="math notranslate nohighlight">\(\langle f(X) \rangle = \sum_x f(x) p(x)\)</span> for discrete random variables, and <span class="math notranslate nohighlight">\(\langle f(X) \rangle = \int f(x) p(x) dx\)</span> for continuous random variables.
In classical statistical mechanics, observables are functions of random variables, and the expected value of the observable is the average value we would expect to measure if we repeated the experiment many times.</p>
<p>(An enterprising reader might ask, isn’t <span class="math notranslate nohighlight">\(Y = f(X)\)</span> itself a random variable?  The answer is yes, and as it happens calculating the expected value of <span class="math notranslate nohighlight">\(Y\)</span> over the probability distribution for <span class="math notranslate nohighlight">\(Y\)</span>  is equivalent to calculating the expected value of <span class="math notranslate nohighlight">\(f(X)\)</span> over the probability distribution for <span class="math notranslate nohighlight">\(X\)</span>.  This is known as the “law of the unconscious statistician”, or LOTUS.)</p>
<p>Finally, we note that we can also calculate probabilities as expectation values using an “indicator function” <span class="math notranslate nohighlight">\(I_A(x)\)</span>, which is 1 if <span class="math notranslate nohighlight">\(x \in A\)</span> and 0 otherwise.  The probability of <span class="math notranslate nohighlight">\(X\)</span> being in the set <span class="math notranslate nohighlight">\(A\)</span> is given by <span class="math notranslate nohighlight">\(P(X \in A) = \langle I_A(X) \rangle\)</span>.</p>
</section>
</section>
<section id="connection-with-free-energy">
<h2>Connection with Free Energy<a class="headerlink" href="#connection-with-free-energy" title="Link to this heading">#</a></h2>
<p>The free energy is a central quantity in statistical mechanics and thermodynamics.   Typically, we write it as the log of the partition function,</p>
<div class="math notranslate nohighlight">
\[
F = - k_B T \log Z =  k_B T \log \int e^{-\beta H(x)} dx,
\]</div>
<p>where <span class="math notranslate nohighlight">\(H(x)\)</span> is the Hamiltonian of the system, <span class="math notranslate nohighlight">\(T\)</span> is the temperature, and <span class="math notranslate nohighlight">\(k_B\)</span> is the Boltzmann constant.</p>
<!-- The partition function is a sum over all possible states of the system, weighted by the Boltzmann factor $e^{-\beta H(x)}$.  The free energy is a measure of the system's ability to do work, and is related to the probability of observing a particular state of the system.  In particular, the probability of observing a state $x$ is given by -->
<p>Similarly, we might evaluate the free energy of a specific state <span class="math notranslate nohighlight">\(A\)</span> by only integrating over that state:</p>
<div class="math notranslate nohighlight">
\[
F_A = - k_B T \log \int_A e^{-\beta H(x)} dx.
\]</div>
<p>(As a concrete example, if <span class="math notranslate nohighlight">\(A\)</span> is an interval in one dimension, the integral would be evaluated as <span class="math notranslate nohighlight">\(\int_a^b e^{-\beta H(x)} dx\)</span>.)
Now, let us consider a free energy difference between two states <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\Delta F = F_A - F_B = - k_B T \log \frac{\int_A e^{-\beta H(x)} dx}{\int_B e^{-\beta H(x)} dx} 
\]</div>
<p>Dividing both the numerator and denominator by <span class="math notranslate nohighlight">\(\int e^{-\beta H(x)} dx\)</span>, and observing that <span class="math notranslate nohighlight">\(p(x) = e^{-\beta H(x)}/ \int e^{-\beta H(x)} dx\)</span>, we find</p>
<div class="math notranslate nohighlight">
\[
\Delta F = - k_B T \log \frac{\int_A p(x) dx}{\int_B p(x) dx} = - k_B T \log \frac{P(A)}{P(B)}.
\]</div>
<p>In other words, the free energy difference between two states is proportional to the log of the ratio of the probabilities of observing those states.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture-04-linear_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Linear Regression and Least Squares</p>
      </div>
    </a>
    <a class="right-next"
       href="lecture-05a-random-in-numpy.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Using Random Numbers in Numpy</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-and-chemistry">Probability and Chemistry</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-non-rigorous-introduction-to-probability">A (Non-Rigorous) Introduction to Probability</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-random-variable">What is a Random Variable?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantifying-probabilities">Quantifying Probabilities</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-distributions">Probability Distributions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-mass-functions-pmfs">Probability Mass Functions (PMFs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-density-functions-pdfs">Probability Density Functions (PDFs)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#changing-variables-changes-the-pdf">Changing Variables Changes the PDF</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sampling-from-a-probability-distribution">Sampling from a Probability Distribution</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doing-things-with-random-variables">Doing things with Random Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-values">Expected Values</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-with-free-energy">Connection with Free Energy</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Thiede
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>