
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Regression and Least Squares &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture-04-linear_regression';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Eigenvectors, Unitary Matrices, and Matrix Decompositions" href="lecture-03-matrix_decompositions.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <img src="_static/logo.png" class="logo__image only-dark pst-js-only" alt="My sample book - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Welcome to 7870
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part 1. Linear Algebra</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="lecture-01-intro_vectors.html">A Deeper look at Vectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-02-coding_matrices_in_python.html">Linear Algebra in Python with Numpy</a></li>
<li class="toctree-l1"><a class="reference internal" href="lecture-03-matrix_decompositions.html">Eigenvectors, Unitary Matrices, and Matrix Decompositions</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Regression and Least Squares</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Flecture-04-linear_regression.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/lecture-04-linear_regression.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression and Least Squares</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-to-linear-regression">Intro to Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares">Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-and-basis-regression">Polynomial and Basis Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-calculating-the-committor-function">Application: Calculating the Committor Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tikhonov-regularization">Tikhonov Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-tikhonov-regularization">Deriving Tikhonov regularization</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression-and-least-squares">
<h1>Linear Regression and Least Squares<a class="headerlink" href="#linear-regression-and-least-squares" title="Link to this heading">#</a></h1>
<section id="learning-objectives">
<h2>Learning Objectives<a class="headerlink" href="#learning-objectives" title="Link to this heading">#</a></h2>
</section>
<section id="intro-to-linear-regression">
<h2>Intro to Linear Regression<a class="headerlink" href="#intro-to-linear-regression" title="Link to this heading">#</a></h2>
<p>In linear regression, we are given a set of data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and we want to find a line that best fits the data.
We consider the case where <span class="math notranslate nohighlight">\(x_i\)</span> is multi-dimensional, with <span class="math notranslate nohighlight">\(m\)</span> features.
Finding the line that best fits the data corresponds to finding an <span class="math notranslate nohighlight">\(m\)</span>-dimensional vector <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
y_i \approx w \cdot x_i + b
\]</div>
<p>To solve this problem, our first step is to write it as a matrix equation.
We introduce the matrix <span class="math notranslate nohighlight">\(X\)</span> with entries</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,m} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,m} \\
\end{bmatrix}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of data points.  We can then write the linear regression problem as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\vdots \\
y_n
\end{bmatrix}
\approx
\begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; \cdots &amp; x_{1,m} \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; \cdots &amp; x_{2,m} \\
1 &amp; x_{3,1} &amp; x_{3,2} &amp; \cdots &amp; x_{3,m} \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; \cdots &amp; x_{n,m} \\
\end{bmatrix}
\begin{bmatrix}
b \\
w_1 \\
w_2 \\
\vdots \\
w_m
\end{bmatrix}
\end{split}\]</div>
<p>or more succinctly as</p>
<div class="math notranslate nohighlight">
\[
Y \approx X W
\]</div>
<p>where <span class="math notranslate nohighlight">\(Y\)</span> is the vector of <span class="math notranslate nohighlight">\(y_i\)</span> values, <span class="math notranslate nohighlight">\(X\)</span> is the matrix of <span class="math notranslate nohighlight">\(x_i\)</span> values, and <span class="math notranslate nohighlight">\(W\)</span> is the vector of <span class="math notranslate nohighlight">\(w_i\)</span> values and <span class="math notranslate nohighlight">\(b\)</span>.</p>
</section>
<section id="least-squares">
<h2>Least Squares<a class="headerlink" href="#least-squares" title="Link to this heading">#</a></h2>
<p>To get the best approximation possible, we will attempt to minimize the length of the difference vector <span class="math notranslate nohighlight">\(Y - X W\)</span>.</p>
<div class="math notranslate nohighlight">
\[
    W_{\text{LS}} = \text{argmin}_W \| X W - Y\|_2^2 = \text{argmin}_W \sum_{i=1}^n |x_i \cdot W - y_i|^2
\]</div>
<p>This strategy is called the method of least squares for pretty obvious reasons.
(The notation <span class="math notranslate nohighlight">\(\| \cdot \|_2\)</span> denotes the Euclidean norm, which is the square root of the sum of the squares of the elements.
So far we have not discussed any other norms, so this notation is redundant, but it will be good for us to get used to it.)</p>
<p>To solve this, we will take the SVD of <span class="math notranslate nohighlight">\(X\)</span> (here we are using the convention that <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are square orthogonal matrices).</p>
<div class="math notranslate nohighlight">
\[
\| X W - Y\|_2^2 = \| U \Sigma V^\dagger W - Y\|_2^2 = \| \Sigma V^\dagger W - U^\dagger Y\|_2^2
\]</div>
<p>Where the second equality follows that (a) <span class="math notranslate nohighlight">\(U\)</span> is an orthogonal unitary and (b) orthogonal matrices preserve the Euclidean norm.
Moreover, for simplicity we can instead minimize over <span class="math notranslate nohighlight">\(Z = V^\dagger W\)</span> rather than <span class="math notranslate nohighlight">\(W\)</span> directly.</p>
<p>Writing the Euclidean norm out explicitly, we have that</p>
<div class="math notranslate nohighlight">
\[
\| X W - Y\|_2^2 = \sum_{i=1}^r |\sigma_i z_i - (U^\dagger Y)_i|^2  + \sum_{i=r+1}^M |(U^\dagger Y)_i|^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(r\)</span> is the number of non-zero singular values of <span class="math notranslate nohighlight">\(X\)</span>.
We can clearly minimize this by setting <span class="math notranslate nohighlight">\(z_i = \sigma_i^{-1} (U^\dagger Y)_i\)</span> for <span class="math notranslate nohighlight">\(i \leq r\)</span> and making an arbitrary choice for <span class="math notranslate nohighlight">\(z_i\)</span> for <span class="math notranslate nohighlight">\(i &gt; r\)</span>.
For simplicity, we will set <span class="math notranslate nohighlight">\(z_i = 0\)</span> for <span class="math notranslate nohighlight">\(i &gt; r\)</span>.</p>
<p>Recalling that <span class="math notranslate nohighlight">\(Z = V^\dagger W\)</span>, implying that <span class="math notranslate nohighlight">\(V Z = W\)</span>, we can write the solution as</p>
<div class="math notranslate nohighlight">
\[
W_{\text{LS}} = V \Sigma^{+} U^\dagger  Y
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma^{+}\)</span> is the matrix with the reciprocals of the non-zero singular values of <span class="math notranslate nohighlight">\(\Sigma\)</span> on the diagonal and zeros elsewhere.</p>
<p>Remark: The matrix <span class="math notranslate nohighlight">\(V \Sigma^{+} U^\dagger\)</span> is called the Moore-Penrose pseudoinverse of <span class="math notranslate nohighlight">\(X\)</span> and is denoted <span class="math notranslate nohighlight">\(X^+\)</span>.  This is a generalization of the inverse of a matrix to non-square matrices
or matrices that are not full rank, and obeys the following properties:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(X X^+ X = X\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(X^+ X X^+ = X^+\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((X X^+)^\dagger = X X^+\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\((X^+ X)^\dagger = X^+ X\)</span></p></li>
</ol>
<section id="polynomial-and-basis-regression">
<h3>Polynomial and Basis Regression<a class="headerlink" href="#polynomial-and-basis-regression" title="Link to this heading">#</a></h3>
<p>The above discussion can be generalized to polynomial regression.
In this case, we have a set of data points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> and we want to find a polynomial of degree <span class="math notranslate nohighlight">\(d\)</span> that best fits the data.
Rather than using the features <span class="math notranslate nohighlight">\(x_i\)</span> directly, we will first take all monomials of degree <span class="math notranslate nohighlight">\(d\)</span> or less of the features, and use them to
build a larger <span class="math notranslate nohighlight">\(X\)</span> matrix.
For example, if <span class="math notranslate nohighlight">\(d = 2\)</span> and <span class="math notranslate nohighlight">\(m = 2\)</span>, we would have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = \begin{bmatrix}
1 &amp; x_{1,1} &amp; x_{1,2} &amp; x_{1,1}^2 &amp; x_{1,1} x_{1,2} &amp; x_{1,2}^2 \\
1 &amp; x_{2,1} &amp; x_{2,2} &amp; x_{2,1}^2 &amp; x_{2,1} x_{2,2} &amp; x_{2,2}^2 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
1 &amp; x_{n,1} &amp; x_{n,2} &amp; x_{n,1}^2 &amp; x_{n,1} x_{n,2} &amp; x_{n,2}^2 \\
\end{bmatrix}
\end{split}\]</div>
<p>We then proceed as before.  More generally, we can consider a basis of functions <span class="math notranslate nohighlight">\(f_i(x)\)</span> and write the regression problem as</p>
<div class="math notranslate nohighlight">
\[
Y \approx X W = \sum_{i=1}^m f_i(x) w_i
\]</div>
<p>where <span class="math notranslate nohighlight">\(f_i(x)\)</span> are the basis functions and <span class="math notranslate nohighlight">\(w_i\)</span> are the coefficients we want to find.
Here, our matrix <span class="math notranslate nohighlight">\(X\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X = \begin{bmatrix}
f_1(x_1) &amp; f_2(x_1) &amp; \cdots &amp; f_m(x_1) \\
f_1(x_2) &amp; f_2(x_2) &amp; \cdots &amp; f_m(x_2) \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
f_1(x_n) &amp; f_2(x_n) &amp; \cdots &amp; f_m(x_n) \\
\end{bmatrix}
\end{split}\]</div>
</section>
<section id="application-calculating-the-committor-function">
<h3>Application: Calculating the Committor Function<a class="headerlink" href="#application-calculating-the-committor-function" title="Link to this heading">#</a></h3>
<p>We consider a system with <span class="math notranslate nohighlight">\(N\)</span> states, which hops from one state withthe following matrix of transition probabilities:</p>
<div class="math notranslate nohighlight">
\[
T_{ij} = \mathbb{P}(X_{t+1} = j | X_t = i)
\]</div>
<p>where <span class="math notranslate nohighlight">\(X_i\)</span> is the state of the system.  The “|” symbol is read as “given” and is used to denote conditional probabilities.
The committor function is a function that tells us the probability that a system will reach a certain set of states (set <span class="math notranslate nohighlight">\(B\)</span>) before another set of states (set <span class="math notranslate nohighlight">\(A\)</span>).</p>
<div class="math notranslate nohighlight">
\[
q_i = \mathbb{P}(\text{reach } B \text{ before } A | X_i)
\]</div>
<p>This is a key quantity in the study of rare events, and is used to calculate the rate of transitions between states in a field known as transition path theory.
The committor function can be calculated by solving the linear regression problem using the transition matrix of the system, which is a matrix that tells us the probability of transitioning from one state to another.
Specifically, the committor function obeys <span class="math notranslate nohighlight">\(q_i = 0\)</span> for <span class="math notranslate nohighlight">\(i \in A\)</span> and <span class="math notranslate nohighlight">\(q_i = 1\)</span> for <span class="math notranslate nohighlight">\(i \in B\)</span>, and satisfies the equation</p>
<div class="math notranslate nohighlight">
\[
q_i = \sum_j T_{ij} q_j
\]</div>
<p>for all other states.  This is is because the probability of reaching either <span class="math notranslate nohighlight">\(B\)</span> is the same as the probability of reaching <span class="math notranslate nohighlight">\(B\)</span> from any of the states that can be reached from <span class="math notranslate nohighlight">\(i\)</span>, weighted by the probability of reaching those states.</p>
<p>To write this as a linear regression problem, denote by <span class="math notranslate nohighlight">\(D\)</span> the set of states that are not in <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span>.  We can rewrite our sum as</p>
<div class="math notranslate nohighlight">
\[
q_i = \sum_{j \in D} T_{ij} q_j + \sum_{j \in A} T_{ij} * 0 + \sum_{j \in B} T_{ij} * 1
\]</div>
<p>implying</p>
<div class="math notranslate nohighlight">
\[
\sum_{j \in D} ( \delta_{ij} - T_{ij}) q_j + \sum_{j \in B} T_{ij}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{ij}\)</span> is the Kronecker delta function, which is 1 if <span class="math notranslate nohighlight">\(i = j\)</span> and 0 otherwise.
We can then write this as a matrix equation</p>
<div class="math notranslate nohighlight">
\[
(I - T^D) q = b
\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the identity matrix, <span class="math notranslate nohighlight">\(T^D\)</span> is the entries of the transition matrik where both <span class="math notranslate nohighlight">\(i\)</span> and <span class="math notranslate nohighlight">\(j\)</span> are in <span class="math notranslate nohighlight">\(D\)</span>, and <span class="math notranslate nohighlight">\(b\)</span> is a vector with entries <span class="math notranslate nohighlight">\(\sum_{j \in B} T_{ij}\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> in <span class="math notranslate nohighlight">\(D\)</span>.</p>
</section>
</section>
<section id="tikhonov-regularization">
<h2>Tikhonov Regularization<a class="headerlink" href="#tikhonov-regularization" title="Link to this heading">#</a></h2>
<p>In practice, we often have to deal with noisy data, which can lead to overfitting.
We also might also have many more parameters than data, or have a matrix with small singular values, leading to numerical instability.
To deal with these issues, we use <em>regularization</em>.  In regularization, we modify the problem to make it more stable or to prevent overfitting,
at the cost of introducing some bias.</p>
<p>One simple way of regularizing a problem is to simply drop singular values that are below a certain threshold.   This is certainly not a bad approach, but maybe we want something a little more gradual that we can tune.</p>
<p>One common approach is to add a penalty term to the least squares problem.  This is called Tikhonov regularization.  Rather than solving our standard least squares problem, we instead solve the problem</p>
<div class="math notranslate nohighlight">
\[
W_{\text{reg}} = \text{argmin}_W \| X W - Y\|_2^2 + \alpha \| W\|_2^2
\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a parameter that we can tune to control the amount of regularization.
This problem can be solved by taking the SVD of <span class="math notranslate nohighlight">\(X\)</span> and solving the following equation</p>
<div class="math notranslate nohighlight">
\[
W_{\text{reg}} = V (\Sigma^2 + \alpha I)^{-1} \Sigma U^\dagger Y
\]</div>
<p>where <span class="math notranslate nohighlight">\(\Sigma^2\)</span> is the matrix with the square of the singular values of <span class="math notranslate nohighlight">\(\Sigma\)</span> on the diagonal and zeros elsewhere.
As we increase the <span class="math notranslate nohighlight">\(\alpha\)</span>, we slowly reduce noise sensitivity, at the cost of introducing more bias.
To choose the best value of <span class="math notranslate nohighlight">\(\alpha\)</span>, one very simple approach is to use the L-curve method.
In this method, we plot the norm of the residual <span class="math notranslate nohighlight">\(\| X W - Y\|_2^2\)</span> as a function of <span class="math notranslate nohighlight">\(\| W\|_2^2\)</span> for different values of <span class="math notranslate nohighlight">\(\alpha\)</span>.
on a log-log plot.  We then look for the point where the curve has the sharpest bend: this is a  good value of <span class="math notranslate nohighlight">\(\alpha\)</span> to choose.</p>
<section id="deriving-tikhonov-regularization">
<h3>Deriving Tikhonov regularization<a class="headerlink" href="#deriving-tikhonov-regularization" title="Link to this heading">#</a></h3>
<p>To derive this expression, we again take the SVD of <span class="math notranslate nohighlight">\(X\)</span>.<br />
For simplicity, this time we assume that <span class="math notranslate nohighlight">\(U\)</span>, <span class="math notranslate nohighlight">\(\Sigma\)</span>, <span class="math notranslate nohighlight">\(V\)</span> are real.
We can then write the least squares problem as</p>
  <!-- and write the quantity we are minimizing as  -->
<div class="math notranslate nohighlight">
\[
\| X W - Y\|_2^2 + \alpha \| W\|_2^2 
= \| U \Sigma V^t W - Y\|_2^2 + \alpha \| W\|_2^2 
= \| \Sigma V^t W - U^t Y\|_2^2 + \alpha \| V^t W\|_2^2
\]</div>
<p>We have again used the fact that both <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are orthogonal matrices.
Again, defining <span class="math notranslate nohighlight">\(Z = V^t W\)</span>, we can write this as</p>
<div class="math notranslate nohighlight">
\[
\| \Sigma Z - U^t Y\|_2^2 + \alpha \| Z\|_2^2
= \sum_{i=1}^M (\sigma_i z_i - (U^t Y)_i)^2 + \alpha  z_i^2
= \sum_{i=1}^M \sigma_i^2 z_i^2 - 2 \sigma_i z_i (U^t Y)_i
 + (U^t Y)_i^2 
\]</div>
<p>To calculate the minimum, we take the derivative with respect to <span class="math notranslate nohighlight">\(z_i\)</span> and set it to zero.  This gives</p>
<div class="math notranslate nohighlight">
\[
2 (\sigma_i^2 + \alpha) z_i = 2 \sigma_i (U^t Y)_i
\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[
z_i = \frac{\sigma_i (U^t Y)_i}{\sigma_i^2 + \alpha}
\]</div>
<p>Writing this as a matrix equation, we have</p>
<div class="math notranslate nohighlight">
\[
Z = \left( \Sigma^2 + \alpha I \right)^{-1} \Sigma U^t Y
\]</div>
<p>This gives us the solution for <span class="math notranslate nohighlight">\(Z\)</span>.  To get the solution for <span class="math notranslate nohighlight">\(W\)</span>, we use the fact that <span class="math notranslate nohighlight">\(Z = V^t W\)</span>, and write</p>
<div class="math notranslate nohighlight">
\[
W_{\text{reg}} = V Z = V \left( \Sigma^2 + \alpha I \right)^{-1} \Sigma U^t Y
\]</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="lecture-03-matrix_decompositions.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Eigenvectors, Unitary Matrices, and Matrix Decompositions</p>
      </div>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-objectives">Learning Objectives</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#intro-to-linear-regression">Intro to Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#least-squares">Least Squares</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-and-basis-regression">Polynomial and Basis Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#application-calculating-the-committor-function">Application: Calculating the Committor Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tikhonov-regularization">Tikhonov Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deriving-tikhonov-regularization">Deriving Tikhonov regularization</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Erik Thiede
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>